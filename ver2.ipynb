{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(filepath):\n",
    "    data = pd.read_csv(filepath)\n",
    "    data = data.drop(columns=[col for col in data.columns if \"Unnamed\" in col], errors='ignore')\n",
    "    data['country'] = data['country'].astype(str)  # Convert 'country' to categorical strings\n",
    "    X = data.drop(columns=['result'])\n",
    "    y = data['result']\n",
    "    categorical_cols = ['country', 'location', 'gender', 'vis_wuhan', 'from_wuhan']\n",
    "    numerical_cols = [col for col in X.columns if col not in categorical_cols]\n",
    "    return X, y, categorical_cols, numerical_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset into training, validation, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def split_data(X, y):\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42, stratify=y)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def create_preprocessor(categorical_cols, numerical_cols):\n",
    "    categorical_transformer = OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False)\n",
    "    numerical_transformer = StandardScaler()\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, numerical_cols),\n",
    "            ('cat', categorical_transformer, categorical_cols)\n",
    "        ]\n",
    "    )\n",
    "    return preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, title='Confusion Matrix', cmap=plt.cm.Blues):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap=cmap, xticklabels=classes, yticklabels=classes)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_val, X_test, y_val, y_test, name):\n",
    "    \"\"\"Evaluate the given model using validation and test sets.\"\"\"\n",
    "    # Validation set evaluation\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    y_val_prob = model.predict_proba(X_val)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    val_roc_auc = roc_auc_score(y_val, y_val_prob) if y_val_prob is not None else \"N/A\"\n",
    "    \n",
    "    print(f\"\\n{name} Validation Accuracy: {val_accuracy:.2f}\")\n",
    "    if y_val_prob is not None:\n",
    "        print(f\"{name} Validation ROC-AUC: {val_roc_auc:.2f}\")\n",
    "    \n",
    "    # Test set evaluation\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "    report = classification_report(y_test, y_pred, target_names=['Recovered', 'Death'], output_dict=True)\n",
    "    roc_auc = roc_auc_score(y_test, y_prob) if y_prob is not None else \"N/A\"\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"\\n{name} Test Classification Report:\\n{classification_report(y_test, y_pred, target_names=['Recovered', 'Death'])}\")\n",
    "    print(f\"{name} Test Accuracy: {accuracy:.2f}\")\n",
    "    if y_prob is not None:\n",
    "        print(f\"{name} Test ROC-AUC: {roc_auc:.2f}\")\n",
    "    plot_confusion_matrix(cm, classes=['Recovered', 'Death'], title=f'{name} Test Confusion Matrix')\n",
    "\n",
    "    return y_prob, roc_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate a specific classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate_classifier(model, param_grid, model_name, preprocessor, X_train, y_train, X_val, X_test, y_val, y_test, results, roc_curves):\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', model)])\n",
    "    grid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_prob, roc_auc = evaluate_model(best_model, X_val, X_test, y_val, y_test, model_name)\n",
    "    results.append({\n",
    "        'model': model_name,\n",
    "        'accuracy': accuracy_score(y_test, best_model.predict(X_test)),\n",
    "        'roc_auc': roc_auc\n",
    "    })\n",
    "    if y_prob is not None:\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "        roc_curves.append((model_name, fpr, tpr, roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main function for executing the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def main(filepath):\n",
    "    X, y, categorical_cols, numerical_cols = load_and_preprocess_data(filepath)\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = split_data(X, y)\n",
    "    preprocessor = create_preprocessor(categorical_cols, numerical_cols)\n",
    "\n",
    "    results = []\n",
    "    roc_curves = []\n",
    "\n",
    "    # K-Nearest Neighbors\n",
    "    print(\"\\n--- K-Nearest Neighbors ---\")\n",
    "    knn_params = {'classifier__n_neighbors': range(1, 21)}\n",
    "    train_and_evaluate_classifier(KNeighborsClassifier(), knn_params, \"KNN\", preprocessor, X_train, y_train, X_val, X_test, y_val, y_test, results, roc_curves)\n",
    "\n",
    "    # Logistic Regression\n",
    "    print(\"\\n--- Logistic Regression ---\")\n",
    "    logistic_params = {'classifier__C': [0.01, 0.1, 1, 10, 100], 'classifier__max_iter': [100, 200, 500]}\n",
    "    train_and_evaluate_classifier(LogisticRegression(random_state=42), logistic_params, \"Logistic Regression\", preprocessor, X_train, y_train, X_val, X_test, y_val, y_test, results, roc_curves)\n",
    "\n",
    "    # Naive Bayes\n",
    "    print(\"\\n--- Naive Bayes ---\")\n",
    "    nb_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', GaussianNB())])\n",
    "    nb_pipeline.fit(X_train, y_train)\n",
    "    y_prob, roc_auc = evaluate_model(nb_pipeline, X_val, X_test, y_val, y_test, \"Naive Bayes\")\n",
    "    results.append({\n",
    "        'model': \"Naive Bayes\",\n",
    "        'accuracy': accuracy_score(y_test, nb_pipeline.predict(X_test)),\n",
    "        'roc_auc': roc_auc\n",
    "    })\n",
    "    if y_prob is not None:\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "        roc_curves.append((\"Naive Bayes\", fpr, tpr, roc_auc))\n",
    "\n",
    "    # Decision Tree\n",
    "    print(\"\\n--- Decision Tree ---\")\n",
    "    dt_params = {'classifier__max_depth': [3, 5, 10, None], 'classifier__min_samples_split': [2, 5, 10]}\n",
    "    train_and_evaluate_classifier(DecisionTreeClassifier(random_state=42), dt_params, \"Decision Tree\", preprocessor, X_train, y_train, X_val, X_test, y_val, y_test, results, roc_curves)\n",
    "\n",
    "    # Support Vector Machine\n",
    "    print(\"\\n--- Support Vector Machine ---\")\n",
    "    svm_params = {'classifier__C': [0.1, 1, 10], 'classifier__gamma': [0.01, 0.1, 1], 'classifier__kernel': ['rbf']}\n",
    "    train_and_evaluate_classifier(SVC(probability=True, random_state=42), svm_params, \"SVM\", preprocessor, X_train, y_train, X_val, X_test, y_val, y_test, results, roc_curves)\n",
    "\n",
    "    # Final Comparison\n",
    "    comparison_df = pd.DataFrame(results)\n",
    "    comparison_df.sort_values(by='roc_auc', ascending=False, inplace=True)\n",
    "    print(\"\\nFinal Model Comparison:\")\n",
    "    print(comparison_df)\n",
    "\n",
    "    # Plot final ROC curves\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for model_name, fpr, tpr, roc_auc in roc_curves:\n",
    "        plt.plot(fpr, tpr, label=f\"{model_name} (AUC = {roc_auc:.2f})\")\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Final ROC Curve Comparison')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main(\"data.csv\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
